{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "import geocoder # to get coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haunted_scrap(z):  \n",
    "    \n",
    "    mp_state = [\"California1\", \"California2\", \"California3\", \"Texas1\", \"Texas2\"]\n",
    "    \n",
    "    file_name = z\n",
    "    zlower = z.lower()\n",
    "    \n",
    "    if z in mp_state:\n",
    "        z = z[:-1]\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    # import url and parse link\n",
    "    \n",
    "    url = 'http://www.theshadowlands.net/places/'+ zlower +'.htm'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # create new empty list\n",
    "    x = []\n",
    "    \n",
    "    # get text and add to the empty list\n",
    "    x = soup.get_text()\n",
    "    \n",
    "    # split text by '\\n\\n'\n",
    "    x = x.rsplit('\\n\\n')\n",
    "    \n",
    "    # the new empty list below will hold the previous text and split again using '-'\n",
    "    x_list = []\n",
    "\n",
    "    for line in x:\n",
    "        name = line.split('-')\n",
    "        x_list.append(name)\n",
    "        \n",
    "    # return x_list (test)\n",
    "    \n",
    "    # these 3 empty lists will hold the clean text and will be converted to the dataframe columns\n",
    "    \n",
    "    x_city = []\n",
    "    x_venue = []\n",
    "    x_venue_add = []\n",
    "    \n",
    "    # the next 3 for loops will fill the 3 lists\n",
    "    # city\n",
    "    for item in x_list:\n",
    "          \n",
    "        try:\n",
    "            item = item[0]\n",
    "            x_city.append(item.title().strip())\n",
    "\n",
    "        except IndexError:\n",
    "            x_city.append(\"null\")\n",
    "    \n",
    "    # venue\n",
    "    for item in x_list:\n",
    "    \n",
    "        try:\n",
    "            item = item[1]\n",
    "            x_venue.append(item.title().strip())\n",
    "\n",
    "        except IndexError:\n",
    "            x_venue.append(\"null\")\n",
    "    \n",
    "\n",
    "    # run lines below to make sure all hgave the same lenght\n",
    "    \n",
    "    # print(len(x_city))\n",
    "    # print(len(x_venue))\n",
    "        \n",
    "    # print(len(new_venue_add))\n",
    "    \n",
    "    # append all lists to a new state dataframe\n",
    "    \n",
    "    x_df = pd.DataFrame(\n",
    "        {'State': z.title().strip(),\n",
    "         'County': x_city,\n",
    "         'City': x_city,\n",
    "         'Venue': x_venue,\n",
    "         'Latitude': \" \",\n",
    "         'Longitude': \" \",\n",
    "        })\n",
    "    \n",
    "    # drop null values - venue column. \n",
    "    indexNames = x_df[x_df[\"Venue\"] == \"null\"].index\n",
    "\n",
    "    x_df.drop(indexNames, inplace = True)\n",
    "\n",
    "    x_df.drop(x_df.index[0], inplace = True)\n",
    "    \n",
    "    # RUNS ONLY FOR TEXAS AND CALIFORNIA:\n",
    "    \n",
    "    if file_name == \"Texas2\":\n",
    "        x_df.drop(x_df.index[0:2], inplace = True)\n",
    "        \n",
    "    if file_name == \"California1\":\n",
    "        x_df.drop(x_df.index[0:3], inplace = True)\n",
    "        \n",
    "    if file_name == \"California2\":\n",
    "        x_df.drop(x_df.index[0:1], inplace = True)\n",
    "        \n",
    "    if file_name == \"California3\":\n",
    "        x_df.drop(x_df.index[0:2], inplace = True)\n",
    "        \n",
    "    # replace '\\n' to ' '\n",
    "    cols_to_check = ['County', 'City', 'Venue']\n",
    "\n",
    "    x_df[cols_to_check] = x_df[cols_to_check].replace({'\\n':' ', '<':'', '>':'' }, regex=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # THE NEXT FEW LINE WILL FILTER AND IMPORT THE COUNTY NAME FROM THE DF (county_data.csv) - see 2nd line on the top\n",
    "    \n",
    "    county_data = pd.read_csv(\"us_counties.csv\")\n",
    "    \n",
    "    county_data[\"County\"] = county_data[\"County\"].str.title()\n",
    "    \n",
    "    county_data = county_data.rename(columns={\"State full\": \"State\"})\n",
    "    \n",
    "    county_data = county_data[(county_data[\"State\"] == z)]\n",
    "    \n",
    "    # create a dictionary only with the current state counties.\n",
    "    \n",
    "    di = dict(zip(county_data.City, county_data.County))\n",
    "    \n",
    "    #replace on the dataframe using .replace()\n",
    "    \n",
    "    x_df = x_df.replace({\"County\": di})\n",
    " \n",
    "\n",
    "    # COLLECT LAT LONG:\n",
    "    \n",
    "    # reset the index, so the Lat/Long can match the correct Cities\n",
    "    x_df = x_df.reset_index()\n",
    "    x_df = x_df.drop('index', axis = 1)\n",
    "    \n",
    "    \n",
    "    # GETTING THE GEO COORDINATES\n",
    "    \n",
    "    # DON'T FORGET TO IMPORT THE CORRECT LIBRARIES:\n",
    "   \n",
    "    # from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "    # import geocoder # to get coordinates\n",
    "    \n",
    "    state_name = x_df['State']\n",
    "    state_name = state_name.values[0]\n",
    "    \n",
    "    # function to get coordinates\n",
    "    def get_latlng(city):\n",
    "        # initialize your variable to None\n",
    "        lat_lng_coords = None\n",
    "        # loop until you get the coordinates\n",
    "        while(lat_lng_coords is None):\n",
    "            g = geocoder.arcgis(city + ',' + state_name)\n",
    "            lat_lng_coords = g.latlng\n",
    "        return lat_lng_coords\n",
    "    \n",
    "    # call the function to get the coordinates, store in a new list using list comprehension\n",
    "    coords = [ get_latlng(city) for city in x_df['City'] ]\n",
    "    \n",
    "    # create temporary dataframe to populate the coordinates into Latitude and Longitude\n",
    "    df_coords = pd.DataFrame(coords, columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # merge the coordinates into the original dataframe\n",
    "    x_df['Latitude'] = df_coords['Latitude']\n",
    "    x_df['Longitude'] = df_coords['Longitude']\n",
    "\n",
    "    # PRINT DATAFRAME HEAD\n",
    "    #print(url)\n",
    "    #return x_df.head(5)\n",
    "    \n",
    "    # SAVE TO CSV FILES\n",
    "    x_df.to_csv(file_name + '_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run function in one state:\n",
    "\n",
    "#haunted_scrap(z = 'Alabama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of states withou spaces. California has 3 pages and Texas 2.\n",
    "\n",
    "state_names = [\"Alaska\", \"Alabama\", \"Arkansas\", \"Arizona\", \"California1\", \"California2\", \"California3\", \"Colorado\", \"Connecticut\", \"DC\", \n",
    "               \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \n",
    "               \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \n",
    "               \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \n",
    "               \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n",
    "               \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas1\", \"Texas2\", \"Utah\", \"Virginia\", \n",
    "               \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in state_names:\n",
    "    haunted_scrap(z = state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put together all tables:\n",
    "\n",
    "mycsvdir = '/Users/\"your_user_name_here\"/Documents/Data Science/Untitled Folder/CSV State Files/'\n",
    "\n",
    "csvfiles = glob.glob(os.path.join(mycsvdir, '*.csv'))\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile)\n",
    "    dataframes.append(df)\n",
    "\n",
    "haunted_df = pd.concat(dataframes, ignore_index = True)\n",
    "\n",
    "haunted_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "haunted_df['State'].replace({'Dc': 'District of Columbia'}, inplace = True)\n",
    "\n",
    "\n",
    "haunted_df.to_csv('haunted_df.csv', index=False)\n",
    "\n",
    "haunted_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
